\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{color}
\usepackage{booktabs}

\begin{document}
\noindent
\large{\textbf{Extract from:}} 
\newline
Bradley Efron and Trevor Hastie
\newline
\textit{Computer Age Statistical Inference: Algorithms, Evidence, and Data Science}
\newline
Cambridge University Press, 2016
\newline
\textit{\url{https://web.stanford.edu/~hastie/CASI\_files/PDF/casi}}
\newline
\newline
\newline


Modern Bayesian practice uses various strategies to construct an
appropriate “prior” $g(\mu)$ in the absence of prior experience, leaving
many statisticians unconvinced by the resulting Bayesian inferences.
Our second example illustrates the difficulty.
\newline
\newline
\noindent
\textcolor{blue}{\textbf{Table 3.1}} \textit{Scores from two tests taken by 22 students,} \textcolor{green}{\textbf{mechanics}}
\textit{and} \textcolor{green}{\textbf{vectors}}.
\newline

\begin{tabular}{cccccccccccc}

     & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\
     \midrule
     \textcolor{green}{\textbf{mechanics}} & 7 & 44 & 49 & 59 & 34 & 46 & 0 & 32 & 49 & 52 & 44 \\
     \textcolor{green}{\textbf{vectors}} & 51 & 69 & 41 & 70 & 42 & 40 & 40 & 45 & 57 & 64 & 61 \\
     \bottomrule
\end{tabular}
\\
\\

\begin{tabular}{cccccccccccc}
     & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19 & 20 & 21 & 22 \\
     \midrule
     \textcolor{green}{\textbf{mechanics}} & 36 & 42 & 5 & 22 & 18 & 41 & 48 & 31 & 42 & 46 & 63 \\
     \textcolor{green}{\textbf{vectors}} & 59 & 60 & 30 & 58 & 51 & 63 & 38 & 42 & 69 & 49 & 63 \\
     \bottomrule
\end{tabular}\\
\\
\\

Table 3.1 shows the scores on two tests, mechanics and vectors,
achieved by $n=22$ students. The sample correlation coefficient between
the two scores is $\hat{\theta}=0.498$,
\begin{center}
  $\hat{\theta}=\sum\limits^{22}_{i=1}(m_i-\overline{m})/[\sum\limits^{22}_{i=1}(m_i-\overline{m})^2\sum\limits^{22}_{i=1}(v_i-\overline{v})^2]^{1/2}$  
\end{center}
with $m$ and $v$ short for \textcolor{green}{\textbf{mechanics}} and \textcolor{green}{\textbf{vectors}}, $\overline{m}$ and $\overline{v}$  their averages.




\end{document}
